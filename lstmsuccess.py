# -*- coding: utf-8 -*-
"""LSTMsuccess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5vln1xY1gu5WDhtepHvqWzPAVG5NWsK
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

df=pd.read_csv('/content/drive/MyDrive/NUS_GAIP_GRP9/HPE Project/Datasets/Merged_food.csv')

# Extract only the time series which have data of all 145 weeks
col_name=df.columns
updated_df=pd.DataFrame(columns=col_name)

for x in df['meal_id'].unique():
  #print('meal_id={}'.format(x))
  for y in df['center_id'].unique():
    #print('center_id={}'.format(y))
    j=0
    temp=df[df['center_id']==y]
    temp=temp[temp['meal_id']==x]
    if temp.empty:
      continue
    if len(temp)!=145:
     #print("skipped{} {}".format(x,y))
      continue
    updated_df=pd.concat([updated_df,temp])

updated_df=pd.read_csv(('/content/drive/MyDrive/NUS_GAIP_GRP9/HPE Project/Datasets/Extracted_food.csv'))
updated_df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)

def calc_mean(col):
  return col.mean()

#Extracting a portion of the dataset
data_new = pd.DataFrame(updated_df.groupby(['center_id','week']).agg({'num_orders':sum,'base_price':calc_mean}))
data_new = data_new.reset_index()
temp = data_new

#Dummy encoding the categorical column
dummy=pd.get_dummies(data_new['center_id'], prefix=['center_id'])

#Merging the newly created dummy columns with the original dataset
data_new = pd.merge(data_new,dummy,on=data_new.index)
data_new.drop(['key_0','center_id'],axis=1,inplace=True)

#Splitting the dataset into train, validation and test sets
train = data_new[data_new['week'].isin(range(1,100))]
val = data_new[data_new['week'].isin(range(100,130))]
test = data_new[data_new['week'].isin(range(130,146))]
print(len(train), len(val), len(test))

'''#Splitting the dataset into X_train, y_train and so on
X_train = train.drop('num_orders',axis=1)
y_train = train['num_orders']

X_val = val.drop('num_orders',axis=1)
y_val = val['num_orders']

X_test = test.drop('num_orders',axis=1)
y_test = test['num_orders']

X_train=np.array(X_train)
X_val=np.array(X_val)
X_test=np.array(X_test)'''

#Performing RObust scaling on the num_orders column
from sklearn.preprocessing import RobustScaler
scaler= RobustScaler()
scaler.fit(np.array(train[['num_orders']]).reshape(-1,1))
train['num_orders'] = scaler.transform(np.array(train[['num_orders']]).reshape(-1,1))
val['num_orders'] = scaler.transform(np.array(val[['num_orders']]).reshape(-1,1))
test['num_orders'] = scaler.transform(np.array(test[['num_orders']]).reshape(-1,1))

#Performing Robust scaling on the base_price column
scaler.fit(np.array(train[['base_price']]).reshape(-1,1))
train['base_price'] = scaler.transform(np.array(train[['base_price']]).reshape(-1,1))
val['base_price'] = scaler.transform(np.array(val[['base_price']]).reshape(-1,1))
test['base_price'] = scaler.transform(np.array(test[['base_price']]).reshape(-1,1))

#Create dataset function to convert the train test and validation sets into numpy arrays
def create_dataset(X, y, time_steps=1, weeks=100):
    Xs, ys = [], []
    total=len(X)//weeks # floor division
    for j in range(total):
      
      for i in range(weeks - time_steps):
        i=i+j*weeks
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)        
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

#Invoking the create dataset function
x_train,y_train=create_dataset(train.drop('num_orders',axis=1), train['num_orders'], time_steps=2, weeks=100)
x_val, y_val=create_dataset(val.drop('num_orders',axis=1), val['num_orders'], time_steps=2, weeks=30)
x_test, y_test=create_dataset(test.drop('num_orders',axis=1), test['num_orders'], time_steps=2, weeks=16)

y_val

#Successful Architecture LSTM
from tensorflow import keras
import tensorflow as tf
from keras.layers import BatchNormalization
from tensorflow.keras import regularizers

model = keras.Sequential()
model.add(
    keras.layers.LSTM(
      units=16, 
      input_shape=(x_train.shape[1], x_train.shape[2]),
      activation = 'sigmoid',
      return_sequences = True
    )
  )

'''model.add(keras.layers.Dropout(rate=0.5))
model.add(BatchNormalization())

model.add(
    keras.layers.LSTM(
      units=32, 
      input_shape=(x_train.shape[1], x_train.shape[2]),
      activation = 'tanh',
      return_sequences = True
    )
  )'''

model.add(keras.layers.Dropout(rate=0.3))
model.add(BatchNormalization())
model.add(
    keras.layers.LSTM(
      units=8, 
      input_shape=(x_train.shape[1], x_train.shape[2]),
      activation = 'tanh',
      return_sequences = False
    )
  )

model.add(keras.layers.Dropout(rate=0.3))
model.add(BatchNormalization())

model.add(keras.layers.Dense(units=16))
model.add(keras.layers.Dropout(0.3))

model.add(keras.layers.Dense(32, kernel_regularizer=regularizers.l2(0.02)))
model.add(keras.layers.Dropout(0.3))

model.add(keras.layers.Dense(units=1))

model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))

'''earlystop=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    mode="min",
    restore_best_weights=True,
)'''
history = model.fit(x=x_train,y=y_train,
          validation_data = (x_val,y_val),
          batch_size=128, epochs=100, callbacks=earlystop)

earlystop=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    mode="min",
    restore_best_weights=True,
)
history = model.fit(x=x_train,y=y_train,
          validation_data = (x_val,y_val),
          batch_size=128, epochs=100, callbacks=earlystop)

model.summary()

model.save_weights('/content/drive/MyDrive/LSTMSuccess1.h5')

history.history

plt.plot(history.history['val_loss'])
plt.plot(history.history['loss'])
plt.legend(['validation','train'])
plt.xlabel('epoch')
plt.ylabel('loss')

y_pred = model.predict(x_val)
y_pred

plt.plot(y_pred)

plt.plot(y_val)

y_pred_test = model.predict(x_test)
y_pred_test

from sklearn.metrics import r2_score
r2_score(y_test, y_pred_test)

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred_test)

plt.plot(y_pred_test)

plt.plot(y_test)

y_pred_test = y_pred_test.flatten()
y_pred_test.shape

plt.scatter(range(len(y_test)),y_test-y_pred_test)