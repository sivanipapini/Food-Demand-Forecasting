# -*- coding: utf-8 -*-
"""ANNsucess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czE_i1HMfKszeURKOwJYaklP7dSolD2p
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf

tf.__version__

#Importing the dataset
df=pd.read_csv('/content/drive/MyDrive/NUS_GAIP_GRP9/HPE Project/Datasets/Merged_food.csv')

# Extract only the time series which have data of all 145 weeks
col_name=df.columns
updated_df=pd.DataFrame(columns=col_name)

for x in df['meal_id'].unique():
  #print('meal_id={}'.format(x))
  for y in df['center_id'].unique():
    #print('center_id={}'.format(y))
    j=0
    temp=df[df['center_id']==y]
    temp=temp[temp['meal_id']==x]
    if temp.empty:
      continue
    if len(temp)!=145:
     #print("skipped{} {}".format(x,y))
      continue
    updated_df=pd.concat([updated_df,temp])

updated_df.to_csv('/content/drive/MyDrive/NUS_GAIP_GRP9/HPE Project/Datasets/Extracted_food.csv')
updated_df

updated_df=pd.read_csv(('/content/drive/MyDrive/NUS_GAIP_GRP9/HPE Project/Datasets/Extracted_food.csv'))

updated_df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)

def calc_mean(col):
  return col.mean()

#Extracting a portion of the dataset
data_new = pd.DataFrame(updated_df.groupby(['center_id','week']).agg({'num_orders':sum,'base_price':calc_mean}))
data_new = data_new.reset_index()
temp = data_new

data_new.columns

#Dummy encoding the categorical column
dummy=pd.get_dummies(data_new['center_id'], prefix=['center_id'])

#Merging the newly created dummy columns with the original dataset
data_new = pd.merge(data_new,dummy,on=data_new.index)

data_new.drop(['key_0','center_id'],axis=1,inplace=True)

#Splitting the dataset into train, validation and test sets
train = data_new[data_new['week'].isin(range(1,100))]
val = data_new[data_new['week'].isin(range(100,130))]
test = data_new[data_new['week'].isin(range(130,146))]
print(len(train), len(val), len(test))

#Performing Robust scaling on the num_orders column
from sklearn.preprocessing import RobustScaler
scaler= RobustScaler()
scaler.fit(np.array(train[['num_orders']]).reshape(-1,1))
train['num_orders'] = scaler.transform(np.array(train[['num_orders']]).reshape(-1,1))
val['num_orders'] = scaler.transform(np.array(val[['num_orders']]).reshape(-1,1))
test['num_orders'] = scaler.transform(np.array(test[['num_orders']]).reshape(-1,1))

dir(scaler)
scaler.__getstate__()

#Performing Robust scaling on the base_price column
scaler.fit(np.array(train[['base_price']]).reshape(-1,1))
train['base_price'] = scaler.transform(np.array(train[['base_price']]).reshape(-1,1))
val['base_price'] = scaler.transform(np.array(val[['base_price']]).reshape(-1,1))
test['base_price'] = scaler.transform(np.array(test[['base_price']]).reshape(-1,1))

X_train = train.drop('num_orders',axis=1)
y_train = train['num_orders']

X_val = val.drop('num_orders',axis=1)
y_val = val['num_orders']

X_test = test.drop('num_orders',axis=1)
y_test = test['num_orders']

X_train=np.array(X_train)
X_val=np.array(X_val)
X_test=np.array(X_test)

#Successful ANN architecture
import tensorflow as tf
inputs= tf.keras.Input(shape=78)
x= tf.keras.layers.Dense(units=16, activation='relu')(inputs)
x= tf.keras.layers.Dense(units=32)(x)
x = tf.keras.layers.Dropout(rate=0.2)(x)
x= tf.keras.layers.Dense(units=16, activation='relu')(x)
x = tf.keras.layers.Dropout(rate=0.2)(x)
x= tf.keras.layers.Dense(units=8)(x)
#layer= tf.keras.layers.CategoryEncoding()
outputs=tf.keras.layers.Dense(units=1)(x)
model= tf.keras.Model(inputs=inputs, outputs=outputs, name='Dense')

model.compile(
    loss="mse",
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
)

'''earlystop=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    mode="min",
    restore_best_weights=True,
)'''


history=model.fit(X_train, y_train, epochs=100,batch_size=64 ,validation_data = (X_val,y_val), callbacks=earlystop)

model.compile(
    loss="mse",
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
)

earlystop=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    mode="min",
    restore_best_weights=True,
)


history=model.fit(X_train, y_train, epochs=100,batch_size=64 ,validation_data = (X_val,y_val), callbacks=earlystop)

plt.plot(history.history['val_loss'])
plt.plot(history.history['loss'])
plt.legend(['validation','train'])
plt.xlabel('epoch')
plt.ylabel('loss')

y_pred = model.predict(X_val)
y_pred

y_pred_test = model.predict(X_test)
y_pred_test

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred_test)

plt.plot(y_pred_test)
plt.legend(['Test Predictions'])

plt.plot(y_test)
plt.legend(['Test Actual Values'])

plt.scatter(range(len(y_test)),y_test-y_pred_test)

model.save_weights('/content/drive/MyDrive/ANNSucess1.h5')

y_pred_test= y_pred_test.flatten()
y_pred